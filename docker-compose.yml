services:
  # Knowledge Graph Database - DSRP patterns and relationships
  typedb:
    image: typedb/typedb:latest
    ports:
      - "1729:1729"
      - "1730:8000"
    volumes:
      - typedb_data:/var/lib/typedb/data
    healthcheck:
      test: ["CMD-SHELL", "echo 'healthy'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Vector Database - Documents, chunks, embeddings, semantic search
  # Replaces MongoDB - unified storage for RAG pipeline
  postgres:
    image: pgvector/pgvector:pg16
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: dsrp
      POSTGRES_PASSWORD: dsrp_password
      POSTGRES_DB: dsrp_canvas
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dsrp -d dsrp_canvas"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Cache Layer - API response caching
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Ollama for Local LLM and Embeddings (optional - can use cloud APIs)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - local-llm  # Only start with: docker-compose --profile local-llm up
    # Uncomment for GPU support (requires nvidia-container-toolkit)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      # AI Providers (at least one required)
      - AI_PROVIDER=${AI_PROVIDER:-anthropic}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Database connections
      - TYPEDB_HOST=typedb
      - TYPEDB_PORT=1729
      - POSTGRES_URL=postgresql://dsrp:dsrp_password@postgres:5432/dsrp_canvas
      - REDIS_URL=redis://redis:6379/0
      # Ollama (optional - for local embeddings)
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    volumes:
      - ./backend:/app
      - ./uploads:/app/uploads
      - ./documents:/app/documents
    depends_on:
      typedb:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # MCP Server (for Claude Desktop integration)
  mcp:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TYPEDB_HOST=typedb
      - TYPEDB_PORT=1729
      - POSTGRES_URL=postgresql://dsrp:dsrp_password@postgres:5432/dsrp_canvas
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./backend:/app
    depends_on:
      - backend
    command: fastmcp run mcp/dsrp_server.py --transport sse --port 8001
    profiles:
      - mcp  # Only start with: docker-compose --profile mcp up

  # React Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - API_PROXY_TARGET=http://backend:8000
      - VITE_API_URL=
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    command: npm run dev -- --host

  # DSRP Knowledge Ingestion Pipeline
  # Processes documents (PDF, video, audio), extracts DSRP patterns
  # Usage: docker-compose --profile tools run pipeline python ingest.py <file>
  pipeline:
    build:
      context: ./pipeline
      dockerfile: Dockerfile
    env_file:
      - .env
    environment:
      # AI Providers
      - AI_PROVIDER=${AI_PROVIDER:-anthropic}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Database connections
      - TYPEDB_HOST=typedb
      - TYPEDB_PORT=1729
      - TYPEDB_DATABASE=dsrp_483
      - TYPEDB_USERNAME=admin
      - TYPEDB_PASSWORD=password
      - POSTGRES_URL=postgresql://dsrp:dsrp_password@postgres:5432/dsrp_canvas
      # Embeddings
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    volumes:
      - ./pipeline:/app
      - ./documents:/app/documents
    depends_on:
      typedb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    command: ["tail", "-f", "/dev/null"]
    profiles:
      - tools  # Only start with: docker-compose --profile tools up

volumes:
  typedb_data:
  postgres_data:
  redis_data:
  ollama_data:
